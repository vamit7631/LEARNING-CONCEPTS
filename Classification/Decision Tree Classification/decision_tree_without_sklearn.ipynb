{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOFGHE6zXbK51B9tWPzfIz0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SirkX2MuWRvE","executionInfo":{"status":"ok","timestamp":1713899168825,"user_tz":-330,"elapsed":1704,"user":{"displayName":"Amit Verma","userId":"07879621006035299738"}},"outputId":"a952161b-1aef-4ed4-ee29-235b9f82e5fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.94\n"]}],"source":["import numpy as np\n","import pandas as pd\n","\n","# Define a class for Decision Tree Node\n","class Node:\n","    def __init__(self, attribute=None, threshold=None, left=None, right=None, value=None):\n","        self.attribute = attribute  # Index of attribute to split on\n","        self.threshold = threshold  # Threshold value for numerical attributes\n","        self.left = left            # Left child\n","        self.right = right          # Right child\n","        self.value = value          # Value if node is a leaf\n","\n","# Function to calculate entropy\n","def entropy(y):\n","    classes, counts = np.unique(y, return_counts=True)\n","    probabilities = counts / len(y)\n","    entropy = -np.sum(probabilities * np.log2(probabilities))\n","    return entropy\n","\n","# Function to calculate information gain\n","def information_gain(X, y, attribute, threshold):\n","    left_indices = X[:, attribute] <= threshold\n","    right_indices = ~left_indices\n","    left_y = y[left_indices]\n","    right_y = y[right_indices]\n","    parent_entropy = entropy(y)\n","    left_entropy = entropy(left_y)\n","    right_entropy = entropy(right_y)\n","    left_weight = len(left_y) / len(y)\n","    right_weight = len(right_y) / len(y)\n","    child_entropy = left_weight * left_entropy + right_weight * right_entropy\n","    info_gain = parent_entropy - child_entropy\n","    return info_gain\n","\n","# Function to find best attribute and threshold to split on\n","def find_best_split(X, y):\n","    best_info_gain = -np.inf\n","    best_attribute = None\n","    best_threshold = None\n","    n_samples, n_features = X.shape\n","    for attribute in range(n_features):\n","        thresholds = np.unique(X[:, attribute])\n","        for threshold in thresholds:\n","            info_gain = information_gain(X, y, attribute, threshold)\n","            if info_gain > best_info_gain:\n","                best_info_gain = info_gain\n","                best_attribute = attribute\n","                best_threshold = threshold\n","    return best_attribute, best_threshold\n","\n","# Function to build the decision tree\n","def build_tree(X, y, depth=0, max_depth=None):\n","    if len(np.unique(y)) == 1 or depth == max_depth:\n","        leaf_value = max(set(y), key=y.tolist().count)\n","        return Node(value=leaf_value)\n","    else:\n","        best_attribute, best_threshold = find_best_split(X, y)\n","        if best_attribute is not None:\n","            left_indices = X[:, best_attribute] <= best_threshold\n","            right_indices = ~left_indices\n","            left_child = build_tree(X[left_indices], y[left_indices], depth+1, max_depth)\n","            right_child = build_tree(X[right_indices], y[right_indices], depth+1, max_depth)\n","            return Node(attribute=best_attribute, threshold=best_threshold, left=left_child, right=right_child)\n","        else:\n","            return Node(value=max(set(y), key=y.tolist().count))\n","\n","# Function to predict using the decision tree\n","def predict(tree, X):\n","    if tree.value is not None:\n","        return tree.value\n","    else:\n","        if X[tree.attribute] <= tree.threshold:\n","            return predict(tree.left, X)\n","        else:\n","            return predict(tree.right, X)\n","\n","# Load dataset\n","dataset = pd.read_csv('Social_Network_Ads.csv')\n","\n","# Extract features and target variable\n","X = dataset.iloc[:, :-1].values\n","y = dataset.iloc[:, -1].values\n","\n","# Split the dataset into training and testing sets\n","def train_test_split(X, y, test_size=0.25, random_state=None):\n","    if random_state is not None:\n","        np.random.seed(random_state)\n","    indices = np.arange(len(X))\n","    np.random.shuffle(indices)\n","    test_size = int(len(X) * test_size)\n","    test_indices = indices[:test_size]\n","    train_indices = indices[test_size:]\n","    X_train, X_test = X[train_indices], X[test_indices]\n","    y_train, y_test = y[train_indices], y[test_indices]\n","    return X_train, X_test, y_train, y_test\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n","\n","# Standardize features\n","def standardize(X_train, X_test):\n","    mean = np.mean(X_train, axis=0)\n","    std = np.std(X_train, axis=0)\n","    X_train = (X_train - mean) / std\n","    X_test = (X_test - mean) / std\n","    return X_train, X_test\n","\n","X_train, X_test = standardize(X_train, X_test)\n","\n","# Build the decision tree\n","tree = build_tree(X_train, y_train, max_depth=3)\n","\n","# Predict\n","y_pred = [predict(tree, x) for x in X_test]\n","\n","# Evaluate\n","def accuracy(y_true, y_pred):\n","    return np.mean(y_true == y_pred)\n","\n","print(\"Accuracy:\", accuracy(y_test, y_pred))\n"]}]}